{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "mount_file_id": "1iMj_v-jJSKJ1sw0Gnwa2Jh3n0TTZ_juZ",
      "authorship_tag": "ABX9TyOOAvMzgq2iOFvrfjhdkWcF"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "## New section"
      ],
      "metadata": {
        "id": "zpmc7owDSUzL"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "This notebook is designed to utilize AWS Bedrock services to perform question-answering tasks using a Retrieval-Augmented Generation (RAG) approach. It employs the 'anthropic.claude-v2' language model.\n",
        "\n",
        "The notebook can be broadly summarized into the following steps:\n",
        "\n",
        "1. Configuration and Initialization:\n",
        "   - Set up the AWS Bedrock client using the provided AWS credentials.\n",
        "   - Initialize the Bedrock language model (LLM) for generating answers and Bedrock embeddings for handling text embeddings.\n",
        "\n",
        "2. Loading and Processing Text Data:\n",
        "   - Load a text file ('askAIevidenceText.txt') which contains the context or evidence to be used for answering questions.\n",
        "   - Process the loaded text, splitting it into manageable chunks that can be individually embedded and retrieved later.\n",
        "\n",
        "3. Embedding Generation:\n",
        "   - Utilize a Sentence Transformer model ('all-MiniLM-L6-v2') to generate embeddings for each chunk of text. These embeddings will later be used to identify the most relevant context for a given question.\n",
        "\n",
        "4. Question Answering (The RAG Process):\n",
        "   - For a given question, the notebook does not explicitly show the retrieval part. However, it's implied that the document embeddings would be used to find the most relevant context.\n",
        "   - Then, using the Claude language model from Anthropic, it generates an answer based on the context provided (or intended to be provided) from the previous step.\n",
        "\n",
        "The embedding model used, 'amazon.titan-embed-text-v1', is a pre-trained model available through AWS Bedrock services that is capable of converting text into high-dimensional vectors (embeddings)."
      ],
      "metadata": {
        "id": "VpmDGpldSs5Y"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yiX3l1maHTEO"
      },
      "outputs": [],
      "source": [
        "!pip install langchain jq sentence-transformers boto3 faiss-cpu\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Create a boto3 client"
      ],
      "metadata": {
        "id": "MY2UZelXkPsH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import json\n",
        "import os\n",
        "import sys\n",
        "import boto3\n",
        "from langchain.llms import Bedrock\n",
        "\n",
        "os.environ[\"AWS_ACCESS_KEY_ID\"]=\"key id here\"\n",
        "os.environ[\"AWS_SECRET_ACCESS_KEY\"]=\"access key here\"\n",
        "\n",
        "boto3_bedrock = boto3.client(\n",
        "    service_name=\"bedrock-runtime\",\n",
        "    region_name=\"us-west-2\",\n",
        "    aws_access_key_id=os.environ.get(\"AWS_ACCESS_KEY_ID\"),\n",
        "    aws_secret_access_key=os.environ.get(\"AWS_SECRET_ACCESS_KEY\"),\n",
        ")\n",
        "llm = Bedrock(model_id=\"anthropic.claude-v2\", client=boto3_bedrock)\n"
      ],
      "metadata": {
        "id": "prCVRx-Ia2WZ"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Creating an embedding using titan"
      ],
      "metadata": {
        "id": "x0sCbhNNb8rh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.embeddings import BedrockEmbeddings\n",
        "\n",
        "bedrock_embeddings = BedrockEmbeddings(model_id=\"amazon.titan-embed-text-v1\", client=boto3_bedrock)"
      ],
      "metadata": {
        "id": "JUmaz8kKblt2"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Loading documents using the TextLoader and split them into manageable chunks creating embeddings\n",
        "\n"
      ],
      "metadata": {
        "id": "YJKqVs6mkZox"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.document_loaders import TextLoader\n",
        "from langchain.text_splitter import CharacterTextSplitter, RecursiveCharacterTextSplitter\n",
        "from pprint import pprint\n",
        "from sentence_transformers import SentenceTransformer\n",
        "from langchain.embeddings import BedrockEmbeddings\n",
        "from langchain.llms.bedrock import Bedrock\n",
        "\n",
        "\n",
        "encoding = 'ISO-8859-1'\n",
        "file_path = \"askAIevidenceText.txt\"\n",
        "loader = TextLoader(file_path, encoding=encoding)\n",
        "\n",
        "model = SentenceTransformer('all-MiniLM-L6-v2')\n",
        "data = loader.load()\n",
        "text_splitter = RecursiveCharacterTextSplitter(\n",
        "                chunk_size = 1000,\n",
        "                chunk_overlap  = 100,\n",
        "                )\n",
        "docs = text_splitter.split_documents(data)\n",
        "# Generate embeddings for all chunks\n",
        "doc_embeddings = model.encode([doc.page_content for doc in docs])\n",
        "\n"
      ],
      "metadata": {
        "id": "ATw5RmDg3iD1"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "trying out embedding by titan..."
      ],
      "metadata": {
        "id": "d_BnGvEnoQMj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "sample_embedding = np.array(bedrock_embeddings.embed_query(docs[0].page_content))\n",
        "print(\"Sample embedding of a document chunk: \", sample_embedding)\n",
        "print(\"Size of the embedding: \", sample_embedding.shape)\n"
      ],
      "metadata": {
        "id": "8NgMVrBibp_B"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Using facebook AI support: Facebook AI Similarity Search"
      ],
      "metadata": {
        "id": "GajQPSTHrdaF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import faiss\n",
        "import numpy as np\n",
        "\n",
        "def create_index(embeddings):\n",
        "    dimension = embeddings.shape[1]  # Get the dimension of the embeddings\n",
        "    index = faiss.IndexFlatL2(dimension)  # Using L2 distance for similarity\n",
        "    index.add(embeddings)  # Add the embeddings to the index\n",
        "    return index\n",
        "\n",
        "def search_index(index, query_embedding, k=5):\n",
        "    # Faiss expects numpy array in float32\n",
        "    query_embedding = query_embedding.astype(np.float32)\n",
        "    # Search for the k nearest neighbors\n",
        "    distances, indices = index.search(query_embedding, k)\n",
        "    return indices\n",
        "\n"
      ],
      "metadata": {
        "id": "Og4mxGOGrN29"
      },
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 1 & 2: Create a FAISS index and add your document embeddings\n",
        "faiss_index = create_index(np.array(doc_embeddings).astype(np.float32))\n",
        "\n",
        "# Step 3 & 4: Create a function to handle new questions\n",
        "def answer_question(question):\n",
        "    # Convert the question to an embedding\n",
        "    question_embedding = model.encode([question])\n",
        "    question_embedding_np = np.array(question_embedding).astype(np.float32)\n",
        "\n",
        "    # Retrieve indices of the relevant docs\n",
        "    indices = search_index(faiss_index, question_embedding_np, k=5)\n",
        "    relevant_docs = [docs[i] for i in indices[0]]\n",
        "\n",
        "    # Step 5: Generate the answer with Claude, using the relevant context\n",
        "    # Join the relevant docs to create a context string\n",
        "    context = \" \".join([doc.page_content for doc in relevant_docs])\n",
        "\n",
        "    # Generate the response with Claude\n",
        "    response = llm.generate(\n",
        "        prompts=[f\"Human: {question} return the answer in a list format \\n\\nRelevant Information: {context}/n Assistant::\"]\n",
        "    )\n",
        "\n",
        "    return response\n",
        "\n",
        "new_question = \"What are the tools used in action?\"\n",
        "answer = answer_question(new_question)\n",
        "print(answer)\n",
        "\n",
        "new_question1 = \"What are the city names mentioned?\"\n",
        "answer1 = answer_question(new_question)\n",
        "print(answer1)\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MbVNGwACj80Y",
        "outputId": "9e587f03-139a-4713-c4e1-4c4540f9b21b"
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "generations=[[Generation(text=' Here are the key tools used in actions mentioned in the evidence:\\n\\n- Small arms/firearms (used in attack on police station)\\n- Sniper rifle (used by sniper in Khost city center attack)  \\n- Explosives (referenced for martyr attacks) \\n- Chlorine gas (referenced as potential WMD)\\n- GSM phones (used for communication)\\n- Email (used for communication/coordination)\\n\\nSo in list format:\\n\\n- Small arms/firearms\\n- Sniper rifle  \\n- Explosives\\n- Chlorine gas\\n- GSM phones\\n- Email')]] llm_output=None run=[RunInfo(run_id=UUID('f59cd3f9-4ecf-45f0-b7b5-5e80869b1ca8'))]\n",
            "generations=[[Generation(text=' - Small arms/firearms\\n- Explosives\\n- Sniper rifles\\n- Mortars\\n- Rockets\\n- Grenades\\n- IEDs (Improvised Explosive Devices)\\n- VBIEDs (Vehicle-Borne IEDs) \\n- Suicide vests\\n- RPGs (Rocket Propelled Grenades)\\n- Missiles\\n- Artillery\\n- Tanks\\n- Technical vehicles (weaponized trucks/cars)\\n- Drones\\n- Heavy machine guns\\n- Night vision devices\\n- Secure communications equipment (radios, phones, etc.)\\n- Reconnaissance equipment\\n- Assault rifles \\n- Ammunition\\n- Melee weapons (knives, machetes, axes)')]] llm_output=None run=[RunInfo(run_id=UUID('b0180295-0f9d-450a-a7d9-1216f02c8352'))]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Load embeddings into FAISS another way with titan\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "lgNYs_vc0RhC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.vectorstores import FAISS\n",
        "import numpy as np\n",
        "from faiss import IndexFlatL2\n",
        "\n",
        "docstore = [doc.page_content for doc in docs]\n",
        "d = 768  # dimension\n",
        "index_faiss = IndexFlatL2(d)  # You may want to use a more sophisticated index in production\n",
        "\n",
        "# The index to docstore ID can just be an identity mapping if your ids are 0-indexed\n",
        "index_to_docstore_id = list(range(len(docs)))\n",
        "\n",
        "# Now initialize the FAISS vector store with the created index, docstore and mapping\n",
        "vectorstore_faiss = FAISS(\n",
        "    embedding_function=lambda x: model.encode([x]),\n",
        "    index=index_faiss,\n",
        "    docstore=docstore,\n",
        "    index_to_docstore_id=index_to_docstore_id\n",
        ")\n",
        "\n",
        "# Add the document embeddings to the index\n",
        "for idx, embedding in enumerate(doc_embeddings):\n",
        "    # FAISS expects the embeddings to be of type float32\n",
        "    vectorstore_faiss.add(np.array(embedding, dtype=np.float32).reshape(1, -1), [idx])\n"
      ],
      "metadata": {
        "id": "pmqWiSew09iL"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}